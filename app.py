import streamlit as st
import pandas as pd
import numpy as np
import io
import os
import requests
import time
from pandasai import SmartDatalake
from pandasai.llm.base import LLM

# è‡ªå®šä¹‰ DeepSeek LLM ç±» - å®Œå…¨ç»•è¿‡ OpenAI
class DeepSeekLLM(LLM):
    def __init__(self, api_key: str, model: str = "deepseek-chat", api_base: str = "https://api.deepseek.com/v1"):
        self.api_key = api_key
        self.model = model
        self.api_base = api_base
        self.max_retries = 3
        self.retry_delay = 2  # ç§’

    def call(self, prompt: str, **kwargs) -> str:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.1,
            "max_tokens": 4000
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if "temperature" in kwargs:
            payload["temperature"] = kwargs["temperature"]
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                response = requests.post(
                    f"{self.api_base}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=30
                )
                
                if response.status_code == 200:
                    return response.json()["choices"][0]["message"]["content"]
                else:
                    error_msg = f"APIé”™è¯¯ ({response.status_code}): {response.text}"
                    if attempt < self.max_retries - 1:
                        time.sleep(self.retry_delay)
                    else:
                        return f"APIè¯·æ±‚å¤±è´¥: {error_msg}"
            
            except requests.exceptions.RequestException as e:
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                else:
                    return f"ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {str(e)}"
        
        return "æœªçŸ¥é”™è¯¯: æ‰€æœ‰é‡è¯•å¤±è´¥"

    @property
    def type(self) -> str:
        return "deepseek-llm"

# --- é¡µé¢è®¾ç½® ---
st.set_page_config(page_title="æ™ºèƒ½æ•°æ®åˆ†æåŠ©ç† (DeepSeekç‰ˆ)", layout="wide")

# --- å…¬ç”¨å‡½æ•° (ç”¨äºæ¨¡æ¿æ›´æ–°åŠŸèƒ½) ---
def update_template_file(template_df: pd.DataFrame, data_dfs: list, key_column: str) -> pd.DataFrame | str:
    if key_column not in template_df.columns: 
        return f"é”™è¯¯ï¼šå…³é”®åˆ— '{key_column}' ä¸å­˜åœ¨äºæ‚¨çš„æ¨¡æ¿æ–‡ä»¶ä¸­ã€‚"
    for df in data_dfs:
        if key_column not in df.columns: 
            return f"é”™è¯¯ï¼šå…³é”®åˆ— '{key_column}' ä¸å­˜åœ¨äºå…¶ä¸­ä¸€ä¸ªæ•°æ®æºæ–‡ä»¶ä¸­ã€‚"
    if not data_dfs: 
        return template_df
    try:
        source_data = pd.concat(data_dfs, ignore_index=True).drop_duplicates(
            subset=[key_column], keep='last').set_index(key_column)
    except Exception as e:
        return f"è®¾ç½®ç´¢å¼•æ—¶å‘ç”Ÿé”™è¯¯: {e}"
    updated_df = template_df.copy()
    for index, row in updated_df.iterrows():
        key_value = row[key_column]
        if key_value in source_data.index:
            source_row = source_data.loc[key_value]
            for col_name in updated_df.columns:
                if pd.isna(row[col_name]):
                    if col_name in source_row.index and not pd.isna(source_row[col_name]):
                        updated_df.loc[index, col_name] = source_row[col_name]
    return updated_df

# --- ä¸»åº”ç”¨ç•Œé¢ ---
st.title("æ™ºèƒ½æ•°æ®åˆ†æåŠ©ç† ğŸš€ (DeepSeek é©±åŠ¨)")
st.markdown("æœ¬åº”ç”¨å·²é…ç½®ä¸ºå®‰å…¨æ¨¡å¼ï¼Œå¯ä¾›å¤šç”¨æˆ·ä½¿ç”¨ã€‚")

# --- ä» Streamlit Secrets å®‰å…¨è·å– DeepSeek API å¯†é’¥ ---
try:
    if "DEEPSEEK_API_KEY" in st.secrets and st.secrets["DEEPSEEK_API_KEY"]:
        api_key = st.secrets["DEEPSEEK_API_KEY"]
        st.sidebar.success("DeepSeek API å¯†é’¥å·²æˆåŠŸåŠ è½½", icon="âœ…")
    else:
        st.error("DeepSeek API å¯†é’¥æœªåœ¨åº”ç”¨çš„ Secrets ä¸­æ­£ç¡®è®¾ç½®ã€‚")
        st.info("å¦‚æœæ‚¨æ˜¯æ­¤åº”ç”¨çš„æ‰€æœ‰è€…ï¼Œè¯·å‰å¾€åº”ç”¨çš„ Settings > Secrets æ·»åŠ æ‚¨çš„ DeepSeek API å¯†é’¥ã€‚")
        st.stop()
except FileNotFoundError:
    st.error("åœ¨æœ¬åœ°è¿è¡Œæ­¤åº”ç”¨æ—¶ï¼Œè¯·åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª .streamlit/secrets.toml æ–‡ä»¶æ¥å­˜æ”¾æ‚¨çš„ DeepSeek API å¯†é’¥ã€‚")
    st.code('# åœ¨ .streamlit/secrets.toml æ–‡ä»¶ä¸­è¿™æ ·å†™:\nDEEPSEEK_API_KEY = "sk-..."')
    st.stop()
except Exception as e:
    st.error(f"åŠ è½½ API å¯†é’¥æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    st.stop()

# --- åˆå§‹åŒ–è‡ªå®šä¹‰ DeepSeek LLM ---
llm = DeepSeekLLM(api_key=api_key)

# --- åŠŸèƒ½åˆ†é¡µ ---
tab1, tab2 = st.tabs(["ğŸ§  æ™ºèƒ½åˆ†æ (è‡ªåŠ¨åˆ†è¡¨)", "ğŸ¯ æ¨¡æ¿ç²¾ç¡®æ›´æ–°"])

# --- æ™ºèƒ½åˆ†æåˆ†é¡µ ---
with tab1:
    st.header("ä¸€æ¬¡æ€§ä¸Šä¼ æ‰€æœ‰æ–‡ä»¶ï¼Œæå‡ºä»»ä½•é—®é¢˜")
    st.success("æœ¬æ¨¡å¼ç”± DeepSeek å¼ºåŠ›é©±åŠ¨ï¼Œå¯ç†è§£å¤šä¸ªä¸åŒæ–‡ä»¶ï¼Œå¹¶æ ¹æ®æ‚¨çš„é—®é¢˜æ™ºèƒ½é€‰æ‹©è¿›è¡Œåˆ†æã€‚")
    
    ai_uploaded_files = st.file_uploader(
        "è¯·ä¸€æ¬¡æ€§ä¸Šä¼ æ‰€æœ‰ç›¸å…³ Excel æ–‡ä»¶ (.xlsx)", type=["xlsx"], 
        accept_multiple_files=True, key="ai_uploader"
    )

    if ai_uploaded_files:
        try:
            ai_dataframes = [pd.read_excel(file) for file in ai_uploaded_files]
            st.success(f"æˆåŠŸä¸Šä¼ å¹¶è¯»å–äº† {len(ai_uploaded_files)} ä¸ªæ–‡ä»¶ï¼")
            
            for i, df in enumerate(ai_dataframes):
                with st.expander(f"é¢„è§ˆæ–‡ä»¶ {i+1}: {ai_uploaded_files[i].name}"):
                    st.dataframe(df.head())

            st.subheader("è¯·è¾“å…¥æ‚¨çš„åˆ†ææŒ‡ä»¤ï¼š")
            user_prompt = st.text_area(
                "æ‚¨å¯ä»¥æå‡ºå…³äºæ˜ç»†çš„å¤æ‚è®¡ç®—é—®é¢˜ï¼Œä¹Ÿå¯ä»¥æå‡ºå…³äºæ±‡æ€»è¡¨çš„æŸ¥è¯¢é—®é¢˜ã€‚",
                key="user_prompt", height=200
            )

            if st.button("ğŸ§  å¼€å§‹æ™ºèƒ½åˆ†æ", type="primary"):
                if user_prompt:
                    with st.spinner("DeepSeek AI æ­£åœ¨è¿›è¡Œæ·±åº¦åˆ†æå’Œè®¡ç®—ï¼Œè¯·ç¨å€™..."):
                        file_descriptions = []
                        for i, df in enumerate(ai_dataframes):
                            file_name = ai_uploaded_files[i].name
                            columns_str = ", ".join([f"'{c}'" for c in df.columns])
                            description = f"- **è¡¨æ ¼{i+1} (æ–‡ä»¶å: '{file_name}')**: åŒ…å«çš„æ ä½æœ‰ [{columns_str}]ã€‚"
                            if "å·¥èµ„" in file_name:
                                description += " (æç¤º: è¿™æ˜¯è–ªé…¬æ˜ç»†è¡¨ï¼Œé€‚åˆç”¨äºè®¡ç®—å…·ä½“è–ªé…¬æ„æˆ)"
                            elif "è´¹ç”¨" in file_name or "å½“æœŸ" in file_name or "ç´¯è®¡" in file_name:
                                description += f" (æç¤º: è¿™æ˜¯è´¹ç”¨æ±‡æ€»è¡¨ï¼Œé€‚åˆç”¨äºæŸ¥è¯¢'{df.columns[1]}'é¡¹ç›®çš„æ€»é¢)"
                            file_descriptions.append(description)
                        
                        context_block = "ä½ æ˜¯ä¸€ä½èµ„æ·±æ•°æ®åˆ†æå¸ˆï¼Œç°åœ¨æœ‰ä»¥ä¸‹å‡ ä¸ªæ•°æ®è¡¨æ ¼ä¾›ä½ ä½¿ç”¨ï¼š\n\n" + "\n".join(file_descriptions)
                        expert_system_prompt = f"""
                        {context_block}
                        ---
                        **ä½ çš„å·¥ä½œè§„åˆ™:**
                        1.  **æ™ºèƒ½é€‰è¡¨ (æœ€é‡è¦)**: åˆ†æç”¨æˆ·çš„è¯·æ±‚ï¼Œä»ä¸Šé¢æä¾›çš„è¡¨æ ¼ä¸­ï¼Œé€‰æ‹©æœ€åŒ¹é…çš„ä¸€ä¸ªæˆ–å¤šä¸ªè¡¨æ ¼æ¥å›ç­”é—®é¢˜ã€‚å¿…é¡»æ ¹æ®è¡¨æ ¼çš„æ ä½å’Œæç¤ºæ¥åšå†³ç­–ã€‚
                        2.  **è®¡ç®—å®¹é”™**: å¦‚æœè®¡ç®—å…¬å¼ä¸­æŸä¸ªæ ä½ä¸å­˜åœ¨ï¼Œè¯·å°†å…¶å½“ä½œ0å¤„ç†ï¼Œä¸è¦æŠ¥é”™ã€‚
                        3.  **ç»“æœå®Œæ•´**: è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰è®¡ç®—ç»“æœçš„å®Œæ•´æ•°æ®è¡¨æ ¼ã€‚
                        ---
                        **ç”¨æˆ·çš„è¯·æ±‚æ˜¯**ï¼šã€{user_prompt}ã€
                        """
                        
                        lake = SmartDatalake(ai_dataframes, config={"llm": llm})
                        result = lake.chat(expert_system_prompt)
                        
                        st.session_state.result = result
                        st.session_state.download_filename = "intelligent_analysis_result.xlsx"
                        st.session_state.download_label = "ğŸ“¥ ä¸‹è½½æ™ºèƒ½åˆ†æç»“æœ (Excel)"
                else:
                    st.warning("è¯·è¾“å…¥æ‚¨çš„åˆ†ææŒ‡ä»¤ã€‚")
        except Exception as e:
            st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")

# --- æ¨¡æ¿ç²¾ç¡®æ›´æ–°åˆ†é¡µ ---
with tab2:
    st.header("å°†å¤šä¸ªæ–‡ä»¶çš„æ•°æ®ï¼Œæ›´æ–°åˆ°ä¸€ä¸ªæ¨¡æ¿æ–‡ä»¶ä¸­")
    st.info("æ­¤åŠŸèƒ½ä¼šä¿æŒæ¨¡æ¿æ–‡ä»¶çš„è¡Œåˆ—é¡ºåºä¸å˜ï¼Œä»…å¡«è¡¥å…¶ä¸­çš„ç©ºç™½å•å…ƒæ ¼ã€‚")
    st.subheader("â‘  ä¸Šä¼ æ‚¨çš„æ¨¡æ¿æ–‡ä»¶")
    template_file = st.file_uploader(
        "è¯·ä¸Šä¼ æ‚¨è¦æ›´æ–°çš„ç›®æ ‡æ¨¡æ¿æ–‡ä»¶ (ä¾‹å¦‚ test.xlsx)", 
        type=["xlsx"], accept_multiple_files=False, key="template_uploader"
    )
    st.subheader("â‘¡ ä¸Šä¼ æ‚¨çš„æ•°æ®æºæ–‡ä»¶")
    data_source_files = st.file_uploader(
        "è¯·ä¸Šä¼ åŒ…å«æ›´æ–°ä¿¡æ¯çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®æ–‡ä»¶", 
        type=["xlsx"], accept_multiple_files=True, key="data_source_uploader"
    )
    st.subheader("â‘¢ è¾“å…¥å…³é”®åˆ—å")
    key_column = st.text_input(
        "è¯·è¾“å…¥ç”¨äºåŒ¹é…æ¨¡æ¿å’Œæ•°æ®æºçš„å­—æ®µåç§°ï¼ˆä¾‹å¦‚ï¼šé¡¹ç›®, ID, å§“åï¼‰", 
        help="æ­¤å­—æ®µå¿…é¡»åŒæ—¶å­˜åœ¨äºæ¨¡æ¿å’Œæ‰€æœ‰æ•°æ®æ–‡ä»¶ä¸­ã€‚"
    )
    if st.button("âš™ï¸ å¼€å§‹ç²¾ç¡®æ›´æ–°", type="primary"):
        if not template_file: 
            st.warning("è¯·ä¸Šä¼ æ¨¡æ¿æ–‡ä»¶ã€‚")
        elif not data_source_files: 
            st.warning("è¯·ä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ•°æ®æºæ–‡ä»¶ã€‚")
        elif not key_column: 
            st.warning("è¯·è¾“å…¥å…³é”®åˆ—åã€‚")
        else:
            with st.spinner("æ­£åœ¨æ‰§è¡Œç²¾ç¡®æ›´æ–°..."):
                try:
                    template_df = pd.read_excel(template_file)
                    data_dfs = [pd.read_excel(file) for file in data_source_files]
                    result = update_template_file(template_df, data_dfs, key_column)
                    st.session_state.result = result
                    st.session_state.download_filename = template_file.name
                    st.session_state.download_label = f"ğŸ“¥ ä¸‹è½½æ›´æ–°åçš„ {template_file.name}"
                except Exception as e: 
                    st.error(f"æ›´æ–°è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

# --- é€šç”¨ç»“æœæ˜¾ç¤ºåŒºåŸŸ ---
st.markdown("---")
if "result" in st.session_state and st.session_state.result is not None:
    st.subheader("ğŸ“ˆ å¤„ç†ç»“æœï¼š")
    result_data = st.session_state.result
    if isinstance(result_data, pd.DataFrame):
        st.dataframe(result_data)
        output_buffer = io.BytesIO()
        with pd.ExcelWriter(output_buffer, engine='openpyxl') as writer: 
            result_data.to_excel(writer, index=False, sheet_name='Result')
        st.download_button(
            label=st.session_state.get("download_label", "ğŸ“¥ ä¸‹è½½ç»“æœ (Excel)"), 
            data=output_buffer.getvalue(), 
            file_name=st.session_state.get("download_filename", "result.xlsx"), 
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    elif isinstance(result_data, str):
        if result_data.startswith("é”™è¯¯ï¼š"): 
            st.error(result_data)
        else: 
            st.write("AI çš„å›å¤æ˜¯æ–‡å­—ï¼Œè€Œä¸æ˜¯è¡¨æ ¼ï¼š")
            st.code(result_data, language=None)
            
st.markdown("---")
st.markdown("ç”± DeepSeek, PandasAI, and Streamlit é©±åŠ¨")