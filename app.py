import streamlit as st
import pandas as pd
import numpy as np
import io
import os
import requests
import time
from pandasai.llm.base import LLM
from pandasai import SmartDatalake
from statsmodels.tsa.statespace.sarimax import SARIMAX
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
import re

# -------------------------------------------------------------------
# æ¨¡å—ä¸€: è‡ªå®šä¹‰çš„ AI æ¨¡å‹æ¥å£ (ä¿æŒä¸å˜)
# -------------------------------------------------------------------
class DeepSeekLLM(LLM):
    """ä¼˜åŒ–çš„DeepSeek LLMé›†æˆç±»"""
    def __init__(self, api_key: str, model: str = "deepseek-chat", temperature: float = 0.1):
        super().__init__()
        self.api_key = api_key; self.model = model; self.temperature = temperature; self.api_base = "https://api.deepseek.com/v1"

    def call(self, prompt: str, *args, **kwargs) -> str:
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {"model": self.model, "messages": [{"role": "user", "content": str(prompt)}], "temperature": self.temperature, "max_tokens": 4096}
        try:
            response = requests.post(f"{self.api_base}/chat/completions", headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            return f"APIè¯·æ±‚æˆ–ç½‘ç»œå‘ç”Ÿé”™è¯¯: {e}"

    @property
    def type(self) -> str:
        return "deepseek-llm"

# -------------------------------------------------------------------
# æ¨¡å—äºŒ: æœ€ç»ˆç‰ˆçš„æ•°æ®å›å¡«å‡½æ•°ï¼ˆå¢åŠ ä¸“å®¶çº§æ¸…æ´—å’Œè°ƒè¯•ï¼‰
# -------------------------------------------------------------------
def fill_template_final(template_df: pd.DataFrame, source_dfs: list, key_columns: list, columns_to_fill: list, 
                      template_path: str = None, inplace: bool = False) -> pd.DataFrame:
    """
    æ ¹æ®å¤šä¸ªå…³é”®åˆ—åŒ¹é…ï¼Œå¯¹æŒ‡å®šåˆ—è¿›è¡Œæ±‚å’Œå¹¶å›å¡«ã€‚
    å‚æ•°:
        template_path: æ¨¡æ¿æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›ä¸”inplace=Trueåˆ™ç›´æ¥ä¿®æ”¹è¯¥æ–‡ä»¶
        inplace: æ˜¯å¦ç›´æ¥ä¿®æ”¹æ¨¡æ¿æ–‡ä»¶
    """
    
    # ä¸“å®¶çº§æ•°æ®æ¸…æ´—å‡½æ•°
    def expert_clean_df(df):
        df_clean = df.copy()
        for col in df_clean.columns:
            # åªå¯¹å­—ç¬¦ä¸²ç±»å‹æ‰§è¡Œå­—ç¬¦ä¸²æ“ä½œ
            if pd.api.types.is_string_dtype(df_clean[col]):
                df_clean[col] = df_clean[col].astype(str).str.strip()
            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
            try:
                df_clean[col] = pd.to_numeric(df_clean[col], errors='ignore')
            except:
                pass
        return df_clean

    cleaned_template_df = expert_clean_df(template_df)
    cleaned_source_dfs = [expert_clean_df(df) for df in source_dfs]

    # æ”¹è¿›å…³é”®åˆ—å¤„ç† - ä¿ç•™åŸå§‹æ ¼å¼ç”¨äºåŒ¹é…
    for df in [cleaned_template_df] + cleaned_source_dfs:
        for key in key_columns:
            if key in df.columns:
                # ä¿ç•™åŸå§‹å€¼ç”¨äºè°ƒè¯•
                df['_original_'+key] = df[key]  
                # æ ‡å‡†åŒ–å¤„ç†
                df[key] = df[key].fillna('N/A').astype(str).str.strip().str.lower().str.replace(r'\s+', '', regex=True)

    valid_sources = [df for df in cleaned_source_dfs if all(key in df.columns for key in key_columns)]
    if not valid_sources:
        raise ValueError("æ•°æ®æºæ–‡ä»¶ä¸­ç¼ºå°‘éƒ¨åˆ†æˆ–å…¨éƒ¨å…³é”®åˆ—ã€‚")
    
    combined_sources = pd.concat(valid_sources, ignore_index=True)
    
    # æ”¹è¿›èšåˆé€»è¾‘ - å¤„ç†å·¥ä½œç°¿6å’Œå·¥ä½œç°¿2çš„ç‰¹æ®Šæƒ…å†µ
    agg_functions = {}
    for col in columns_to_fill:
        if col in combined_sources.columns:
            if pd.api.types.is_numeric_dtype(combined_sources[col]):
                if len(source_dfs) == 2:  # å½“æœ‰ä¸¤ä¸ªæ•°æ®æºæ—¶ï¼ˆå·¥ä½œç°¿6å’Œå·¥ä½œç°¿2ï¼‰
                    # æ£€æŸ¥å·¥ä½œç°¿6å’Œå·¥ä½œç°¿2çš„æ•°æ®æƒ…å†µ
                    df6_has_data = not source_dfs[0][col].isna().all()
                    df2_has_data = not source_dfs[1][col].isna().all()
                    
                    if df6_has_data and not df2_has_data:
                        # å·¥ä½œç°¿6æœ‰æ•°æ®è€Œå·¥ä½œç°¿2æ²¡æœ‰æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨å·¥ä½œç°¿6çš„æ•°æ®
                        agg_functions[col] = ('first', lambda x: x.iloc[0])
                    elif df6_has_data and df2_has_data:
                        # ä¸¤è€…éƒ½æœ‰æ•°æ®ï¼Œå°†ä¸¤åˆ—æ•°æ®ç›¸åŠ 
                        agg_functions[col] = ('sum', lambda x: x.sum())
                    else:
                        # å…¶ä»–æƒ…å†µä½¿ç”¨é»˜è®¤çš„sum
                        agg_functions[col] = 'sum'
                else:
                    # é»˜è®¤æƒ…å†µä½¿ç”¨sum
                    agg_functions[col] = 'sum'
            else:
                agg_functions[col] = 'first'  # ä¿ç•™ç¬¬ä¸€ä¸ªéç©ºå€¼
    
    if not agg_functions:
        return cleaned_template_df
    
    # è¯†åˆ«å¹¶ä¿ç•™ç‰¹æ®Šè¡Œï¼ˆå¦‚"é¡¹ç›®"ã€"ä¸€"ç­‰ï¼‰
    special_rows = combined_sources[
        combined_sources[key_columns[0]].astype(str).str.contains('é¡¹ç›®|ä¸€|â€”', regex=True)
    ]
    
    # æ‰§è¡Œå¸¸è§„èšåˆ
    aggregated_source = combined_sources.groupby(key_columns, as_index=False).agg(agg_functions)
    
    # åˆå¹¶ç‰¹æ®Šè¡Œå’Œèšåˆç»“æœ
    aggregated_source = pd.concat([special_rows, aggregated_source]).drop_duplicates(subset=key_columns, keep='first')
    
    # --- è°ƒè¯•ä¿¡æ¯å°†ç›´æ¥æ˜¾ç¤º ---
    st.markdown("---")
    st.subheader("âš™ï¸ è°ƒè¯•è¯Šæ–­ä¿¡æ¯ (è¯·ä»”ç»†æ¯”å¯¹ä»¥ä¸‹ä¸¤ä¸ªè¡¨æ ¼)")
    st.write("**1. æ¨¡æ¿æ–‡ä»¶å…³é”®åˆ— (æ¸…æ´—åç”¨äºåŒ¹é…çš„å€¼):**")
    st.dataframe(cleaned_template_df[key_columns].head())
    st.write("**2. æºæ•°æ®å…³é”®åˆ— (æ¸…æ´—å¹¶èšåˆåç”¨äºåŒ¹é…çš„å€¼):**")
    st.dataframe(aggregated_source[key_columns].head())
    st.markdown("---")
    
    for key in key_columns:
        cleaned_template_df[key] = cleaned_template_df[key].astype(str)
        
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    st.write("**åˆå¹¶å‰æ¨¡æ¿æ•°æ®æ ·æœ¬:**")
    st.dataframe(cleaned_template_df.head())
    st.write("**åˆå¹¶å‰åˆ†æç»“æœæ ·æœ¬:**")
    st.dataframe(aggregated_source.head())
    
    # æ”¹è¿›åˆå¹¶é€»è¾‘ - å¤„ç†éå”¯ä¸€é”®
    # å…ˆä¸ºæ¨¡æ¿æ•°æ®æ·»åŠ ä¸´æ—¶ç´¢å¼•åˆ—
    cleaned_template_df['_temp_index'] = range(len(cleaned_template_df))
    
    # æ‰§è¡Œåˆå¹¶ï¼Œä¸éªŒè¯ä¸€å¯¹ä¸€å…³ç³»
    # æ”¹è¿›åˆå¹¶é€»è¾‘ - æ·»åŠ è°ƒè¯•ä¿¡æ¯
    st.write("**å…³é”®åˆ—åŒ¹é…è¯¦æƒ…:**")
    st.write(f"æ¨¡æ¿æ•°æ®å…³é”®åˆ—å€¼: {cleaned_template_df[key_columns].head().to_dict()}")
    st.write(f"æºæ•°æ®å…³é”®åˆ—å€¼: {aggregated_source[key_columns].head().to_dict()}")
    
    # æ‰§è¡Œåˆå¹¶å¹¶æ˜¾ç¤ºæœªåŒ¹é…è®°å½•
    merged_df = pd.merge(
        cleaned_template_df,
        aggregated_source,
        on=key_columns,
        how='left',
        suffixes=('', '_source'),
        indicator=True  # æ·»åŠ åˆå¹¶æ ‡è®°
    )
    
    # æ˜¾ç¤ºæœªåŒ¹é…çš„è®°å½•
    unmatched = merged_df[merged_df['_merge'] == 'left_only']
    if not unmatched.empty:
        st.warning(f"å‘ç° {len(unmatched)} æ¡æœªåŒ¹é…è®°å½•")
        st.dataframe(unmatched[key_columns + ['_merge']])
    
    merged_df = merged_df.drop(columns=['_merge'])  # ç§»é™¤åˆå¹¶æ ‡è®°
    
    # æŒ‰åŸå§‹é¡ºåºæ¢å¤å¹¶åˆ é™¤ä¸´æ—¶ç´¢å¼•
    merged_df = merged_df.sort_values('_temp_index').drop(columns=['_temp_index'])
    
    # æ£€æŸ¥åˆå¹¶ç»“æœ
    st.write("**åˆå¹¶åæ•°æ®æ ·æœ¬:**")
    st.dataframe(merged_df.head())

    for col in columns_to_fill:
        source_col_name = f"{col}_source"
        if source_col_name in merged_df.columns:
            # æ”¹è¿›çš„ç©ºç™½æ›¿æ¢é€»è¾‘ - æ”¾å®½æ¡ä»¶
            # åªè¦æºæ•°æ®æœ‰å€¼ä¸”æ¨¡æ¿å€¼ä¸ºç©º/0/None/ç©ºå­—ç¬¦ä¸²ï¼Œå°±è¿›è¡Œæ›¿æ¢
            if pd.api.types.is_numeric_dtype(merged_df[col].dtype):
                mask_to_fill = (merged_df[col].isna()) | (merged_df[col] == 0) | (merged_df[col].astype(str) == "None") | (merged_df[col].astype(str) == "nan")
            else:
                mask_to_fill = (merged_df[col].isna()) | (merged_df[col] == "") | (merged_df[col].astype(str) == "None") | (merged_df[col].astype(str) == "nan")
            
            # ç¡®ä¿æºæ•°æ®æœ‰å€¼æ—¶æ‰æ›¿æ¢
            mask_to_fill = mask_to_fill & (~merged_df[source_col_name].isna())
            
            # æ‰§è¡Œæ›¿æ¢
            merged_df[col] = np.where(
                mask_to_fill,
                merged_df[source_col_name],
                merged_df[col]
            )
            merged_df.drop(columns=[source_col_name], inplace=True)

    final_df = merged_df[template_df.columns]
    for col in final_df.select_dtypes(include=np.number).columns:
        final_df[col] = final_df[col].fillna(0)
    
    # å¦‚æœæŒ‡å®šäº†æ¨¡æ¿è·¯å¾„ä¸”éœ€è¦ç›´æ¥ä¿®æ”¹
    if template_path and inplace:
        try:
            # è·å–åŸå§‹æ–‡ä»¶è·¯å¾„
            original_path = os.path.abspath(template_path)
            
            # æ£€æŸ¥æ¨¡æ¿æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(original_path):
                st.error(f"æ‰¾ä¸åˆ°æ¨¡æ¿æ–‡ä»¶: {original_path}")
                st.info("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
                st.info("ç¡®ä¿æ–‡ä»¶å­˜åœ¨ä¸”å…·æœ‰è¯»å–æƒé™")
                st.info("æˆ–è€…é‡æ–°ä¸Šä¼ æ¨¡æ¿æ–‡ä»¶")
                return final_df
                
            # åˆ›å»ºä¸´æ—¶å‰¯æœ¬(ä½¿ç”¨.xlsxæ‰©å±•å)
            temp_path = os.path.splitext(original_path)[0] + "_temp.xlsx"
            
            # å†™å…¥ä¿®æ”¹åçš„æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶
            with pd.ExcelWriter(temp_path, engine='openpyxl') as writer:
                final_df.to_excel(writer, index=False)
            
            # æ›¿æ¢åŸå§‹æ–‡ä»¶
            if os.path.exists(original_path):
                os.remove(original_path)
            os.rename(temp_path, original_path)
            
            st.success(f"å·²æˆåŠŸä¿®æ”¹æœ¬åœ°æ–‡ä»¶: {original_path}")
        except Exception as e:
            st.error(f"ç›´æ¥ä¿®æ”¹æ–‡ä»¶å¤±è´¥: {e}")
            if os.path.exists(temp_path):
                os.remove(temp_path)
        
    return final_df

# -------------------------------------------------------------------
# æ–°å¢æ¨¡å—: é€šç”¨ç»Ÿè®¡å’Œé¢„æµ‹åŠŸèƒ½
# -------------------------------------------------------------------
# é€šç”¨æ•°æ®è·å–å‡½æ•°ï¼Œæ”¯æŒé“¾æ¥å’Œæ–‡ä»¶ä¸Šä¼ 
def get_general_data(data_source, data_type, data_files=None):
    if data_type == "é‡‘å±ä»·æ ¼":
        if data_source:
            try:
                url = data_source
                response = requests.get(url)
                soup = BeautifulSoup(response.text, 'html.parser')
                high_prices = []
                low_prices = []
                high_price_elements = soup.find_all('span', class_='high-price')
                low_price_elements = soup.find_all('span', class_='low-price')
                for high, low in zip(high_price_elements, low_price_elements):
                    high_prices.append(float(high.text.strip()))
                    low_prices.append(float(low.text.strip()))
                dates = pd.date_range(end=pd.Timestamp.now(), periods=len(high_prices))
                df = pd.DataFrame({
                    'Date': dates,
                    'High Price': high_prices,
                    'Low Price': low_prices
                })
                df['Average Price'] = (df['High Price'] + df['Low Price']) / 2
                return df
            except Exception as e:
                st.error(f"ä»é“¾æ¥è·å–æ•°æ®å¤±è´¥: {str(e)}")
        elif data_files:
            try:
                dfs = [pd.read_excel(f) for f in data_files]
                df = pd.concat(dfs, ignore_index=True)
                if 'High Price' in df.columns and 'Low Price' in df.columns:
                    df['Average Price'] = (df['High Price'] + df['Low Price']) / 2
                    return df
                else:
                    st.error("ä¸Šä¼ æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—ï¼ˆHigh Price å’Œ Low Priceï¼‰")
            except Exception as e:
                st.error(f"ä»ä¸Šä¼ æ–‡ä»¶è·å–æ•°æ®å¤±è´¥: {str(e)}")
    # å¯ä»¥æ·»åŠ å…¶ä»–æ•°æ®ç±»å‹çš„è·å–é€»è¾‘
    st.error(f"æš‚ä¸æ”¯æŒ {data_type} æ•°æ®ç±»å‹çš„è·å–")
    return pd.DataFrame()

# é€šç”¨æ•°æ®é¢„æµ‹å‡½æ•°
def general_predict(df, target_column):
    model = SARIMAX(df[target_column], order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))
    model_fit = model.fit()
    future_steps_1day = 1
    future_steps_1month = 30
    future_steps_1quarter = 90
    future_steps_6months = 180
    forecast_1day = model_fit.get_forecast(steps=future_steps_1day)
    forecast_1month = model_fit.get_forecast(steps=future_steps_1month)
    forecast_1quarter = model_fit.get_forecast(steps=future_steps_1quarter)
    forecast_6months = model_fit.get_forecast(steps=future_steps_6months)
    return forecast_1day.predicted_mean, forecast_1month.predicted_mean, forecast_1quarter.predicted_mean, forecast_6months.predicted_mean

# é€šç”¨å›¾è¡¨ç”Ÿæˆå‡½æ•°
def general_plot(df, forecasts, target_column, data_type):
    plt.figure(figsize=(15, 8))
    plt.plot(df['Date'], df[target_column], label=f'{data_type} å†å²æ•°æ®')
    future_dates = {}
    steps = [30, 90, 180]
    periods = ['1 Month', '1 Quarter', '6 Months']
    for i, forecast in enumerate(forecasts[1:]):
        future_dates[periods[i]] = pd.date_range(start=df['Date'].iloc[-1], periods=len(forecast) + 1, freq='D')[1:]
        plt.plot(future_dates[periods[i]], forecast, label=f'{periods[i]} Forecast', linestyle='--')

    changes = df[target_column].pct_change()
    for i in range(1, len(df)):
        if abs(changes.iloc[i]) > 0.1:
            event = find_event(df['Date'].iloc[i], data_type)
            plt.annotate(f'{event}',
                         xy=(df['Date'].iloc[i], df[target_column].iloc[i]),
                         xytext=(10, 10),
                         textcoords='offset points',
                         arrowprops=dict(arrowstyle='->'))

    plt.title(f'{data_type} ç»Ÿè®¡å’Œé¢„æµ‹')
    plt.xlabel('æ—¥æœŸ')
    plt.ylabel('æ•°å€¼')
    plt.legend()
    plt.grid(True)
    return plt

# æŸ¥æ‰¾å¯¹åº”äº‹ä»¶ï¼ˆç¤ºä¾‹ï¼Œéœ€æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
def find_event(date, data_type):
    return f'{data_type} äº‹ä»¶äº {date.strftime("%Y-%m-%d")}'

# å¯èƒ½çš„äº‹ä»¶ç±»å‹åˆ—è¡¨
def possible_general_events(data_type):
    if data_type == "é‡‘å±ä»·æ ¼":
        return [
            "å…¨çƒç»æµå½¢åŠ¿å˜åŒ–",
            "é‡‘å±ä¾›éœ€å…³ç³»æ”¹å˜",
            "æ”¿ç­–æ³•è§„è°ƒæ•´",
            "è‡ªç„¶ç¾å®³å½±å“ç”Ÿäº§",
            "åœ°ç¼˜æ”¿æ²»å†²çª"
        ]
    # å¯ä»¥æ·»åŠ å…¶ä»–æ•°æ®ç±»å‹çš„äº‹ä»¶åˆ—è¡¨
    return [f"{data_type} ç›¸å…³çš„å¸‚åœºå˜åŒ–", f"{data_type} ç›¸å…³çš„æ”¿ç­–è°ƒæ•´"]

# -------------------------------------------------------------------
# ä¸»ç¨‹åº: Streamlit ç•Œé¢é€»è¾‘ (ä¿®æ”¹ç‰ˆ)
# -------------------------------------------------------------------
def main():
    st.set_page_config(page_title="æ™ºèƒ½æ•°æ®åˆ†æå¹³å°", page_icon="ğŸ§ ", layout="wide")
    st.title("ğŸ§  æ™ºèƒ½æ•°æ®åˆ†æå¹³å°")

    with st.sidebar:
        st.header("ğŸ”‘ API å¯†é’¥")
        api_key = st.text_input("DeepSeek APIå¯†é’¥", type="password", help="ä»DeepSeekå®˜ç½‘è·å–APIå¯†é’¥")

    st.header("AI æ™ºèƒ½åˆ†æ")
    st.info("ä¸Šä¼ æ•°æ®æ–‡ä»¶å¹¶ç”¨è‡ªç„¶è¯­è¨€æé—®ï¼ŒAIä¼šè‡ªåŠ¨åˆ†æå¹¶å¯å°†ç»“æœå¡«å…¥æŒ‡å®šæ¨¡æ¿ã€‚")

    st.header("é€šç”¨ç»Ÿè®¡å’Œé¢„æµ‹")
    data_type = st.selectbox("é€‰æ‹©æ•°æ®ç±»å‹", ["é‡‘å±ä»·æ ¼", "å…¶ä»–ç±»å‹1", "å…¶ä»–ç±»å‹2"])  # å¯æŒ‰éœ€æ‰©å±•
    data_source = st.text_input("è¾“å…¥æ•°æ®æ¥æºé“¾æ¥", "https://example.chinametal.com")
    data_files = st.file_uploader("ä¸Šä¼ æ•°æ®æ–‡ä»¶", accept_multiple_files=True)

    if st.button("å¼€å§‹ç»Ÿè®¡å’Œé¢„æµ‹"):
        with st.spinner("æ­£åœ¨è·å–æ•°æ®å’Œè¿›è¡Œé¢„æµ‹..."):
            try:
                df = get_general_data(data_source, data_type, data_files)
                if df.empty:
                    return
                if data_type == "é‡‘å±ä»·æ ¼":
                    target_column = "Average Price"
                else:
                    # å¯æ ¹æ®ä¸åŒæ•°æ®ç±»å‹æŒ‡å®šç›®æ ‡åˆ—
                    target_column = "Value"  
                forecasts = general_predict(df, target_column)
                fig = general_plot(df, forecasts, target_column, data_type)
                st.pyplot(fig)
                st.write(f"ç¬¬äºŒå¤© {target_column} é¢„æµ‹:", forecasts[0].iloc[0])
                st.write(f"å¯èƒ½å¯¼è‡´ {data_type} å¤§å¹…å˜åŒ–çš„äº‹ä»¶ç±»å‹:", possible_general_events(data_type))
                st.write("é¢„æµ‹åŸç†ï¼šä½¿ç”¨SARIMAXæ—¶é—´åºåˆ—æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œç»“åˆå†å²æ•°æ®çš„è¶‹åŠ¿å’Œå­£èŠ‚æ€§ç‰¹å¾ã€‚")
            except Exception as e:
                st.error(f"ç»Ÿè®¡å’Œé¢„æµ‹å¤±è´¥: {str(e)}")

    # ä¿®å¤æœªå®šä¹‰å˜é‡é—®é¢˜
    data_files = st.file_uploader("ä¸Šä¼ åˆ†æç”¨æ•°æ®æ–‡ä»¶", accept_multiple_files=True)
    prompt = st.text_area("è¾“å…¥åˆ†æéœ€æ±‚", "")
    template_option = st.selectbox("é€‰æ‹©æ¨¡æ¿å¤„ç†æ–¹å¼", ["æ— æ¨¡æ¿", "ä¸Šä¼ æ¨¡æ¿æ–‡ä»¶", "æŒ‡å®šæœ¬åœ°æ–‡ä»¶è·¯å¾„"])
    if template_option == "ä¸Šä¼ æ¨¡æ¿æ–‡ä»¶":
        template_file = st.file_uploader("ä¸Šä¼ æ¨¡æ¿æ–‡ä»¶", type=["xlsx", "xls"])
    elif template_option == "æŒ‡å®šæœ¬åœ°æ–‡ä»¶è·¯å¾„":
        local_path = st.text_input("è¾“å…¥æœ¬åœ°æ¨¡æ¿æ–‡ä»¶è·¯å¾„")
    else:
        template_file = None
        local_path = None

    if st.button("å¼€å§‹åˆ†æ", type="primary", use_container_width=True):
        if not api_key or not api_key.startswith("sk-"):
            st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„APIå¯†é’¥ã€‚")
        elif not data_files:
            st.warning("è¯·ä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ•°æ®æ–‡ä»¶ã€‚")
        elif not prompt.strip():
            st.warning("è¯·è¾“å…¥æ‚¨çš„åˆ†æéœ€æ±‚ã€‚")
        else:
            with st.spinner("AI æ­£åœ¨åˆ†æä¸­..."):
                try:
                    # è¯»å–æ•°æ®æ–‡ä»¶
                    dfs = [pd.read_excel(f) for f in data_files]
                    
                    # åˆå§‹åŒ–AIæ¨¡å‹
                    llm = DeepSeekLLM(api_key=api_key)
                    lake = SmartDatalake(dfs, config={"llm": llm})
                    
                    # æ‰§è¡Œåˆ†æ
                    result = lake.chat(prompt)
                    
                    # æ˜¾ç¤ºåˆ†æç»“æœ
                    st.subheader("åˆ†æç»“æœ")
                    if isinstance(result, pd.DataFrame):
                        st.dataframe(result)
                        
                        # å¤„ç†æ¨¡æ¿æ–‡ä»¶
                        if template_option == "ä¸Šä¼ æ¨¡æ¿æ–‡ä»¶" and template_file:
                            template_df = pd.read_excel(template_file)
                            # ä½¿ç”¨fill_template_finalå‡½æ•°å¤„ç†
                            result_df = fill_template_final(
                                template_df, [result], 
                                key_columns=template_df.columns.tolist()[:1],  # ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºå…³é”®åˆ—
                                columns_to_fill=result.columns.tolist()
                            )
                            
                            # æä¾›ä¸‹è½½æˆ–ç›´æ¥ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
                            if local_path:
                                try:
                                    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨å¹¶å…·æœ‰å†™å…¥æƒé™
                                    dir_path = os.path.dirname(local_path)
                                    if not os.path.exists(dir_path):
                                        os.makedirs(dir_path)
                                    
                                    # åˆ›å»ºå¤‡ä»½æ–‡ä»¶
                                    backup_path = f"{local_path}.bak"
                                    if os.path.exists(local_path):
                                        os.rename(local_path, backup_path)
                                    
                                    # å†™å…¥ç»“æœåˆ°æŒ‡å®šè·¯å¾„
                                    with pd.ExcelWriter(local_path, engine='openpyxl') as writer:
                                        result_df.to_excel(writer, index=False)
                                    st.success(f"ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {local_path}")
                                    
                                    # ä¿ç•™å¤‡ä»½æ–‡ä»¶3ç§’ååˆ é™¤
                                    time.sleep(3)
                                    if os.path.exists(backup_path):
                                        os.remove(backup_path)
                                except Exception as e:
                                    st.error(f"ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„å¤±è´¥: {str(e)}")
                                    if os.path.exists(backup_path) and not os.path.exists(local_path):
                                        os.rename(backup_path, local_path)
                            else:
                                # æä¾›ä¸‹è½½
                                output = io.BytesIO()
                                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                                    result_df.to_excel(writer, index=False)
                                st.download_button(
                                    "ä¸‹è½½å¡«å…¥æ¨¡æ¿çš„ç»“æœ", 
                                    output.getvalue(), 
                                    "analysis_result_in_template.xlsx"
                                )
                        
                        # å¤„ç†æœ¬åœ°æ–‡ä»¶è·¯å¾„
                        elif template_option == "æŒ‡å®šæœ¬åœ°æ–‡ä»¶è·¯å¾„" and local_path:
                            try:
                                # è§„èŒƒåŒ–è·¯å¾„å¹¶æ£€æŸ¥å­˜åœ¨æ€§
                                normalized_path = os.path.normpath(local_path)
                                
                                # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
                                if not os.path.exists(normalized_path):
                                    # å°è¯•åœ¨templatesç›®å½•ä¸‹æŸ¥æ‰¾
                                    templates_path = os.path.join(os.path.dirname(__file__), 'templates', os.path.basename(normalized_path))
                                    if os.path.exists(templates_path):
                                        normalized_path = templates_path
                                    else:
                                        # æ›´å‹å¥½çš„é”™è¯¯æç¤ºï¼ŒåŒ…å«å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ
                                        st.error(f"æ‰¾ä¸åˆ°æ¨¡æ¿æ–‡ä»¶: {normalized_path}\n\nè§£å†³æ–¹æ¡ˆ:\n1. è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®\n2. ç¡®ä¿æ–‡ä»¶å·²æ”¾å…¥'templates'ç›®å½•\n3. æˆ–è€…é‡æ–°ä¸Šä¼ æ¨¡æ¿æ–‡ä»¶")
                                        return
                                
                                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                                if not normalized_path.lower().endswith(('.xlsx', '.xls')):
                                    st.error(f"ä»…æ”¯æŒExcelæ–‡ä»¶(.xlsx/.xls)ï¼Œå½“å‰æ–‡ä»¶: {os.path.splitext(normalized_path)[1]}")
                                    return
                                
                                template_df = pd.read_excel(normalized_path)
                                # ç›´æ¥ä¿®æ”¹æœ¬åœ°æ–‡ä»¶
                                result_df = fill_template_final(
                                    template_df, [result],
                                    key_columns=template_df.columns.tolist()[:1],
                                    columns_to_fill=result.columns.tolist(),
                                    template_path=normalized_path,
                                    inplace=True
                                )
                                st.success(f"æˆåŠŸå¤„ç†æ–‡ä»¶: {normalized_path}")
                            except Exception as e:
                                st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}\nè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®æˆ–æ˜¯å¦è¢«å…¶ä»–ç¨‹åºå ç”¨")
                    else:
                        st.write(result)
                        
                except Exception as e:
                    st.error(f"åˆ†æå¤±è´¥: {str(e)}")

if __name__ == "__main__":
    main()