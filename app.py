import streamlit as st
import pandas as pd
import numpy as np
import io
import os
import requests
import time
from pandasai import SmartDatalake
from pandasai.llm.base import LLM

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•°åŒºåŸŸ ---

# 1. å¼ºåŠ›æ•°æ®æ¸…æ´—å‡½æ•°
def clean_and_convert_to_numeric(df: pd.DataFrame) -> pd.DataFrame:
    """å°†DataFrameä¸­çœ‹èµ·æ¥åƒæ•°å­—çš„æ–‡æœ¬åˆ—å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ç±»å‹"""
    df_clean = df.copy()
    for col in df_clean.columns:
        if df_clean[col].dtype == 'object':
            # å°è¯•ç§»é™¤åƒä½åˆ†éš”ç¬¦ç­‰éæ•°å­—å­—ç¬¦ï¼Œç„¶åè½¬æ¢
            try:
                cleaned_series = df_clean[col].astype(str).str.replace(r'[^\d.-]', '', regex=True)
                # å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ï¼Œæ— æ³•è½¬æ¢çš„å€¼ä¼šå˜æˆNaNï¼ˆNot a Numberï¼‰
                numeric_series = pd.to_numeric(cleaned_series, errors='coerce')
                # åªæœ‰å½“è½¬æ¢åè‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆæ•°å­—æ—¶ï¼Œæ‰æ›¿æ¢åŸå§‹åˆ—
                if not numeric_series.isna().all():
                    df_clean[col] = numeric_series
            except Exception:
                # å¦‚æœåœ¨è½¬æ¢ä¸­å‘ç”Ÿä»»ä½•æ„å¤–ï¼Œåˆ™ä¿æŒè¯¥åˆ—ä¸å˜
                pass
    return df_clean

# 2. æœ€ç»ˆç‰ˆçš„æ•°æ®å›å¡«å‡½æ•°
def fill_template_final(template_df: pd.DataFrame, source_dfs: list, key_columns: list) -> pd.DataFrame:
    """æ ¹æ®å¤šä¸ªå…³é”®åˆ—åŒ¹é…ï¼Œå¯¹æ•°å€¼åˆ—æ±‚å’Œï¼Œå¹¶å›å¡«åˆ°æ¨¡æ¿ä¸­"""
    
    # é¦–å…ˆå¯¹æ‰€æœ‰è¾“å…¥æ–‡ä»¶è¿›è¡Œæ•°æ®æ¸…æ´—
    cleaned_template_df = clean_and_convert_to_numeric(template_df)
    cleaned_source_dfs = [clean_and_convert_to_numeric(df) for df in source_dfs]

    # éªŒè¯å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
    if not all(key in cleaned_template_df.columns for key in key_columns):
        missing_keys = [key for key in key_columns if key not in cleaned_template_df.columns]
        raise ValueError(f"é”™è¯¯ï¼šå…³é”®åˆ— {missing_keys} åœ¨æ¨¡æ¿æ–‡ä»¶ä¸­ä¸å­˜åœ¨ã€‚")

    # ç»Ÿä¸€å…³é”®åˆ—çš„æ•°æ®ç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œä»¥ç¡®ä¿åŒ¹é…æˆåŠŸ
    for df in [cleaned_template_df] + cleaned_source_dfs:
        for key in key_columns:
            if key in df.columns:
                df[key] = df[key].astype(str).str.strip()

    # åˆå¹¶æ‰€æœ‰æœ‰æ•ˆçš„æ•°æ®æº
    all_sources = [df.copy() for df in cleaned_source_dfs if all(key in df.columns for key in key_columns)]
    if not all_sources:
        raise ValueError("é”™è¯¯ï¼šæ‰€æœ‰æ•°æ®æºæ–‡ä»¶ä¸­éƒ½ä¸å®Œæ•´åŒ…å«æ‚¨è¾“å…¥çš„å…¨éƒ¨å…³é”®åˆ—ã€‚")
    
    combined_sources = pd.concat(all_sources, ignore_index=True)
    
    # å®šä¹‰èšåˆè§„åˆ™ï¼šæ•°å€¼åˆ—æ±‚å’Œï¼Œå…¶ä»–åˆ—å–ç¬¬ä¸€ä¸ª
    agg_functions = {}
    for col in combined_sources.columns:
        if col not in key_columns:
            if pd.api.types.is_numeric_dtype(combined_sources[col]):
                agg_functions[col] = 'sum'
            else:
                agg_functions[col] = 'first'
    
    if not agg_functions: return cleaned_template_df
    
    # æŒ‰å¤šå…³é”®åˆ—åˆ†ç»„å¹¶èšåˆ
    aggregated_source = combined_sources.groupby(key_columns).agg(agg_functions).reset_index()
    source_data_lookup = aggregated_source.set_index(key_columns)

    # è¿­ä»£å¹¶å¡«å……æ¨¡æ¿
    filled_df = cleaned_template_df.copy()
    for index, row in filled_df.iterrows():
        key_values = tuple(str(row.get(key, '')) for key in key_columns)
        
        if key_values in source_data_lookup.index:
            source_row = source_data_lookup.loc[key_values]
            for col_name in filled_df.columns:
                is_empty_or_zero = pd.isna(row[col_name]) or (np.isscalar(row[col_name]) and isinstance(row[col_name], (int, float, np.number)) and row[col_name] == 0)
                if is_empty_or_zero:
                    if col_name in source_row.index and not pd.isna(source_row[col_name]):
                        filled_df.loc[index, col_name] = source_row[col_name]
    return filled_df

# 3. AI è°ƒç”¨ç±» (ä¿æŒä¸å˜)
class DeepSeekLLM(LLM):
    def __init__(self, api_key: str, model: str = "deepseek-chat"):
        self.api_key = api_key; self.model = model; self.api_base = "https://api.deepseek.com/v1"
    def call(self, prompt: str, *args, **kwargs) -> str:
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {"model": self.model, "messages": [{"role": "user", "content": str(prompt)}], "temperature": 0.0}
        try:
            response = requests.post(f"{self.api_base}/chat/completions", headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            return f"APIè¯·æ±‚æˆ–ç½‘ç»œå‘ç”Ÿé”™è¯¯: {e}"
    @property
    def type(self) -> str: return "deepseek-llm"


# --- Streamlit åº”ç”¨ä¸»ç•Œé¢ ---
st.set_page_config(page_title="æ™ºèƒ½æ•°æ®å¤„ç†ä¸åˆ†æå¹³å°", page_icon="ğŸ“Š", layout="wide")
st.title("ğŸ“Š æ™ºèƒ½æ•°æ®å¤„ç†ä¸åˆ†æå¹³å°")

with st.sidebar:
    st.header("ğŸ”‘ API å¯†é’¥")
    api_key = st.text_input("DeepSeek APIå¯†é’¥", type="password", help="ä»DeepSeekå®˜ç½‘è·å–APIå¯†é’¥")

tab1, tab2 = st.tabs(["âœï¸ æ•°æ®å›å¡« (æœ€ç»ˆç‰ˆ)", "ğŸ§  æ™ºèƒ½åˆ†æ"])

# æ•°æ®å›å¡«åˆ†é¡µ
with tab1:
    st.header("å°†æºæ•°æ®å›å¡«åˆ°æ¨¡æ¿æ–‡ä»¶")
    st.info("æ­¤æœ€ç»ˆç‰ˆåŠŸèƒ½ä¼šè‡ªåŠ¨æ·±åº¦æ¸…æ´—æ•°æ®ï¼Œå¹¶æ ¹æ®å¤šä¸ªå…³é”®åˆ—åŒ¹é…ï¼Œå°†æ•°å€¼æ±‚å’Œåå¡«å……ã€‚")
    st.markdown("---")

    col1, col2 = st.columns(2)
    with col1:
        source_files = st.file_uploader("1. ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®æºæ–‡ä»¶", type=["xlsx", "xls"], accept_multiple_files=True, key="source_files")
    with col2:
        template_file = st.file_uploader("2. ä¸Šä¼ ä¸€ä¸ªç©ºç™½çš„æ¨¡æ¿æ–‡ä»¶", type=["xlsx", "xls"], key="template_file")
    
    key_column_input = st.text_input("3. è¯·è¾“å…¥ä¸€ä¸ªæˆ–å¤šä¸ªå…³é”®åˆ—åï¼Œç”¨è‹±æ–‡é€—å·éš”å¼€", "éƒ¨é—¨,è´¹ç”¨é¡¹ç›®", help="ä¾‹å¦‚ï¼šéƒ¨é—¨,è´¹ç”¨é¡¹ç›®")

    if st.button("å¼€å§‹å›å¡«", type="primary", use_container_width=True, key="start_fill_data"):
        if not source_files or not template_file or not key_column_input.strip():
            st.warning("è¯·ç¡®ä¿å·²ä¸Šä¼ æ•°æ®æºã€æ¨¡æ¿æ–‡ä»¶ï¼Œå¹¶å·²è¾“å…¥å…³é”®åˆ—åã€‚")
        else:
            with st.spinner("æ­£åœ¨æ·±åº¦æ¸…æ´—ã€åˆ†ç»„æ±‚å’Œå¹¶å›å¡«æ•°æ®..."):
                try:
                    key_columns_list = [key.strip() for key in key_column_input.split(',')]
                    source_dfs = [pd.read_excel(f) if f.name.endswith('xlsx') else pd.read_csv(f) for f in source_files]
                    template_df = pd.read_excel(template_file) if template_file.name.endswith('xlsx') else pd.read_csv(template_file)
                    
                    filled_df = fill_template_final(template_df, source_dfs, key_columns_list)
                    
                    st.success("æ•°æ®å›å¡«æˆåŠŸï¼")
                    st.dataframe(filled_df)
                    
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        filled_df.to_excel(writer, index=False, sheet_name="Final_Filled_Template")
                    excel_data = output.getvalue()
                    
                    st.download_button(label="ğŸ“¥ ä¸‹è½½å·²å›å¡«çš„æ¨¡æ¿æ–‡ä»¶", data=excel_data, file_name=f"final_filled_{template_file.name}")
                except Exception as e:
                    st.error(f"å›å¡«è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

# æ™ºèƒ½åˆ†æåˆ†é¡µ
with tab2:
    st.header("ä½¿ç”¨ AI è¿›è¡Œé€šç”¨çš„ã€æ¢ç´¢æ€§çš„æ•°æ®åˆ†æ")
    if not api_key or not api_key.startswith("sk-"): st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„DeepSeek APIå¯†é’¥ä»¥å¯ç”¨â€œæ™ºèƒ½åˆ†æâ€åŠŸèƒ½ã€‚")
    ai_uploaded_files_tab2 = st.file_uploader("ä¸Šä¼ ç”¨äºAIåˆ†æçš„Excelæ–‡ä»¶", type=["xlsx", "xls"], accept_multiple_files=True, key="ai_uploader_tab2")
    if ai_uploaded_files_tab2 and api_key.startswith("sk-"):
        analysis_prompt = st.text_area("æ‚¨çš„åˆ†æéœ€æ±‚", height=100, key="analysis_input")
        if st.button("å¼€å§‹æ™ºèƒ½åˆ†æ", type="primary", use_container_width=True, key="start_ai_analysis"):
            if analysis_prompt.strip():
                with st.spinner("AIæ­£åœ¨æ€è€ƒä¸­..."):
                    try:
                        llm = DeepSeekLLM(api_key=api_key)
                        data_frames = [pd.read_excel(f) for f in ai_uploaded_files_tab2]
                        lake = SmartDatalake(data_frames, config={"llm": llm})
                        response = lake.chat(analysis_prompt)
                        st.subheader("åˆ†æç»“æœ")
                        st.write(response)
                    except Exception as e: st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼š\n\n{e}")